# IRON: iron_headers
#
# Distribution A
#
# Approved for Public Release, Distribution Unlimited
#
# EdgeCT (IRON) Software Contract No.: HR0011-15-C-0097
# DCOMP (GNAT)  Software Contract No.: HR0011-17-C-0050
# Copyright (c) 2015-20 Raytheon BBN Technologies Corp.
#
# This material is based upon work supported by the Defense Advanced
# Research Projects Agency under Contracts No. HR0011-15-C-0097 and
# HR0011-17-C-0050. Any opinions, findings and conclusions or
# recommendations expressed in this material are those of the author(s)
# and do not necessarily reflect the views of the Defense Advanced
# Research Project Agency.
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
#
# IRON: end

#
# This file lists additional BPF configuration parameters that can be
# edited but are generally unnecessary or unsafe, for reasons described
# in the documentation herein.
#

################# QUEUING RULES #################################
#
# The queue depth computation policy intended for proxy packet admission.
#
# Set to true for 'max' and false for 'sum'.
#
Bpf.Alg.Mcast.MaxAdmission  true

#
# The dequeue policy for all bins in the BinQueueMgr. May be "FIFO" or "LIFO".
#
# Code is essentially untested with LIFO.
#
Bpf.BinQueueMgr.DequeuePolicy  FIFO

#
# The drop policy for all bins in the BinQueueMgr.  May be "HEAD", "TAIL", or
# "NO_DROP".
#
# With "NO_DROP" (the default), enqueus will fail if the queue is at capacity.
# This is mostly a moot point for IRON, since the system is designed to avoid
# queue overflow.
#
Bpf.BinQueueMgr.DropPolicy  NO_DROP

################# HEAVYBALL #####################################
# HeavyBall has not been tested in conjunction with other IRON
# features, and we have other mechanisms for reducing latency.
# Therefore, this feature is essentially sandboxed.

#
# The heavyball weight update interval in microsecs.
#
Bpf.HvyBall.WeightComputationIntervalUsec 5000

#
# The heavyball beta value. Higher numbers mean more queue depth smoothing and
# lower latency at the expense of reaction time.
#
Bpf.HvyBall.Beta 0.65


################# NO PACKET LEFT BEHIND #########################
# No packet left behind is a mechanism for avoiding destination starvation
# when a destination has a low load. We have observed problems with this
# algorithm in conjunction with IRON admission control, and thus developed
# AntiStarvationZombies as an alternate (and preferred) method to avoid
# starvation.

# Used only if NPLB is the queue depth manager.
#
# This is how much weight to place on the queue-delay term in the
# backpressure gradients. This will be equally weighted to the queue depth
# term when set to [drain-rate / 1x10^6], since the delay term reflects
# how long a packet has been sitting first in the queue in micro seconds,
# and the queue depths are in bytes.
#
# The default value is 0.1. A value of 1 is approximately equal to the queue
# depth bytes weight if the drain rate is 10 Mbps.
#
Bpf.QueueDelayWeight 0.1

#
# True if we want to generate on-the-fly XPLOT graphs of the per-destination
# no packet left behind values, including queue depth along, delay, and the
# sum of the two. This still won't generate graphs unless the XPLOT compile
# option is enabled.
#
# Default is false.
#
Bpf.GenerateNPLBGraphs false

################# LATENCY-AWARE FORWARDING ######################

#
# If true, queueing delay estimates will be included when estimating
# minimum latency to destination for the sake of Latency-Aware forwarding
# path-pruning.
#
# This is not recommended: our findings show that queue delays are too
# variable to be accurately estimated, especially since there is a delay
# in terms of getting the latency estimates into the latency-to-destination
# computations. (During that delay, the queue delay changes.)
#
Bpf.Laf.IncludeQueuingDelays false

#
# Drop expired packets instead of Zombifying them.
#
# Default value is false. Using true is not recommended because it creates
# a leak in the backpressure system: if packets are dropped when they expire,
# the queue appears short, so admission control will add more packets, which
# again are unlikely to make it to the destination.
#
Bpf.Alg.DropExpired false

# TODO
Bpf.Alg.Mcast.ExcludeInfinitePaths  true


################# ZOMBIE LATENCY REDUCTION ######################

#
# Queue change rate threshold below which we will NOT add zombie packets for
# the sake of latency reduction. i.e., if the queue depth for a bin is
# increasing at a rate less than this (if this is negative, that would mean
# dequeues are happening faster than enqueues), we will not replace dequeued/
# sent packets with zombies.
# The default value is -2000 Bytes per second.
# Decreasing this will make the system less responsive to system dynamics.
# Increasing this will increase latency.
#
# This value logically should have little to no effect with the current ZLR
# algorithm.
#
Bpf.ZLR.QChangeMinThreshBytesPerS -2000

# True if we want to drop zombies when we dequeue them instead of forwarding
# to the neighbor.
#
# Default value is false. We have discovered that dropping zombies immediately
# instead of transmitting them causes big queue depth oscillations, because
# there is not rate control on how quickly zombie packets can be
# dequeued. Quickly dequeuing lots of zombies leads to an instantaneous
# decrease in queue depth, which causes a huge influx of packets, causing a
# steep increase in queue depth.  By dropping zombies when they are received
# instead of when they are dequeued, we get the same effect of not wasting
# (much) capacity, but with an implicit rate control.
#
Bpf.DropDequeuedZombies false


################# OSCILLATION REDUCTION #########################

#
# If true, resets to the queue depth oscillations will just remove all the
# existing samples and start period computations from scratch. If false,
# resets will also stop smoothing until we have a new period computed.
# Default value is false.
#
# We have seen little benefit to soft resets. When the oscillation period is
# incorrect, the queue depths end up way off - soft resets don't fix this.
#
Bpf.Osc.UseSoftReset false

################ MULTICAST ######################################
#
# If true, group advertisement messages (GRAMs) will be creates and sent
# on the GRAM multicast group. This is not needed if all the multicast
# groups are statically defined in the bin map, but necessary for dynamic
# multicast group management.
# Default is true.

Bpf.SendGrams true


################# BBN DEMOWARE ##################################

#
# The remote control TCP port number.
# This is used by the BBN demo to control and show statistics from the BPF
# from the demo GUI.
#
Bpf.RemoteControl.Port  5560

################# CAPACITY ESTIMATES ############################

#
# The boolean to include reporting network-wide capacity estimates.
# This is used to allow LSAs to include link capacity estimates and return them
# via remote control message queries.
#
Bpf.IncludeLinkCapacity true
